{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb527614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "LA ÚLTIMA INOCENCIA — Alejandra Pizarnik\n",
      "Tokens (N): 10\n",
      "Types  (V): 10\n",
      "=========================================\n",
      "Fragmento del poema XXVI — Dulce María Loynaz\n",
      "Tokens (N): 11\n",
      "Types  (V): 11\n",
      "=========================================\n",
      "Ambos textos (combinado)\n",
      "Tokens (N): 21\n",
      "Types  (V): 16\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Conteo de tokens y types (sin librerías)\n",
    "# =========================================\n",
    "# Reglas de tokenización:\n",
    "# - Un token es toda secuencia continua de letras (incluye acentos) o dígitos.\n",
    "# - Todo lo demás (espacios, signos, guiones, comillas, etc.) separa tokens.\n",
    "# - Los \"types\" se cuentan en minúsculas (no se quitan acentos).\n",
    "#\n",
    "# Cómo usar:\n",
    "# 1) Pega el contenido de cada texto en las variables text_pizarnik y text_loynaz.\n",
    "# 2) Ejecuta este archivo. Verás el total de tokens y types por cada texto.\n",
    "\n",
    "# ====== Pega aquí los textos ======\n",
    "text_pizarnik = \"\"\"[PEGA AQUÍ el poema \"La última inocencia\" de Alejandra Pizarnik]\"\"\"\n",
    "\n",
    "text_loynaz = \"\"\"[PEGA AQUÍ el fragmento del poema XXVI de Dulce María Loynaz]\"\"\"\n",
    "# ==================================\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    \"\"\"\n",
    "    Devuelve la lista de tokens de s usando la regla:\n",
    "    secuencias de letras (Unicode) o dígitos -> token.\n",
    "    Todo lo demás es separador.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    current = []\n",
    "\n",
    "    for ch in s:\n",
    "        if ch.isalpha() or ch.isdigit():\n",
    "            current.append(ch)\n",
    "        else:\n",
    "            if current:\n",
    "                tokens.append(\"\".join(current))\n",
    "                current = []\n",
    "    # Si quedó algo al final, añadirlo\n",
    "    if current:\n",
    "        tokens.append(\"\".join(current))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def count_tokens_and_types(s):\n",
    "    tokens = tokenize(s)\n",
    "    # normalizamos a minúsculas para el conteo de types\n",
    "    types = set(t.lower() for t in tokens if t != \"\")\n",
    "    return len(tokens), len(types)\n",
    "\n",
    "\n",
    "def report_for(title, text):\n",
    "    n_tokens, n_types = count_tokens_and_types(text)\n",
    "    print(\"=========================================\")\n",
    "    print(title)\n",
    "    print(\"Tokens (N):\", n_tokens)\n",
    "    print(\"Types  (V):\", n_types)\n",
    "\n",
    "\n",
    "def main():\n",
    "    report_for(\"LA ÚLTIMA INOCENCIA — Alejandra Pizarnik\", text_pizarnik)\n",
    "    report_for(\"Fragmento del poema XXVI — Dulce María Loynaz\", text_loynaz)\n",
    "\n",
    "    # (Opcional) Reporte combinado\n",
    "    both = text_pizarnik + \"\\n\" + text_loynaz\n",
    "    n_tokens, n_types = count_tokens_and_types(both)\n",
    "    print(\"=========================================\")\n",
    "    print(\"Ambos textos (combinado)\")\n",
    "    print(\"Tokens (N):\", n_tokens)\n",
    "    print(\"Types  (V):\", n_types)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bef6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "LA ÚLTIMA INOCENCIA — Alejandra Pizarnik\n",
      "Tokens (N): 42\n",
      "Types  (V): 30\n",
      "=========================================\n",
      "Fragmento del poema XXVI — Dulce María Loynaz\n",
      "Tokens (N): 92\n",
      "Types  (V): 49\n",
      "=========================================\n",
      "Ambos textos (combinado)\n",
      "Tokens (N): 134\n",
      "Types  (V): 71\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Conteo de tokens y types (sin librerías)\n",
    "# Lee los textos desde archivo\n",
    "# =========================================\n",
    "\n",
    "# Función para tokenizar\n",
    "def tokenize(s):\n",
    "    tokens = []\n",
    "    current = []\n",
    "    for ch in s:\n",
    "        if ch.isalpha() or ch.isdigit():\n",
    "            current.append(ch)\n",
    "        else:\n",
    "            if current:\n",
    "                tokens.append(\"\".join(current))\n",
    "                current = []\n",
    "    if current:\n",
    "        tokens.append(\"\".join(current))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def count_tokens_and_types(s):\n",
    "    tokens = tokenize(s)\n",
    "    types = set(t.lower() for t in tokens if t != \"\")\n",
    "    return len(tokens), len(types)\n",
    "\n",
    "\n",
    "def report_for(file_path, title):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    n_tokens, n_types = count_tokens_and_types(text)\n",
    "    print(\"=========================================\")\n",
    "    print(title)\n",
    "    print(\"Tokens (N):\", n_tokens)\n",
    "    print(\"Types  (V):\", n_types)\n",
    "\n",
    "\n",
    "def main():\n",
    "    report_for(\"ultimaInocencia.txt\", \"LA ÚLTIMA INOCENCIA — Alejandra Pizarnik\")\n",
    "    report_for(\"DulceMaríaLoynaz.txt\", \"Fragmento del poema XXVI — Dulce María Loynaz\")\n",
    "\n",
    "    # Opcional: conteo combinado\n",
    "    with open(\"ultimaInocencia.txt\", \"r\", encoding=\"utf-8\") as f1, \\\n",
    "         open(\"DulceMaríaLoynaz.txt\", \"r\", encoding=\"utf-8\") as f2:\n",
    "        both = f1.read() + \"\\n\" + f2.read()\n",
    "\n",
    "    n_tokens, n_types = count_tokens_and_types(both)\n",
    "    print(\"=========================================\")\n",
    "    print(\"Ambos textos (combinado)\")\n",
    "    print(\"Tokens (N):\", n_tokens)\n",
    "    print(\"Types  (V):\", n_types)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "TEXTO: La última inocencia — Alejandra Pizarnik\n",
      "Tokens totales: 47\n",
      "Lemas totales : 31\n",
      "Distribución POS:\n",
      "  NOUN: 13\n",
      "  ADP: 7\n",
      "  CCONJ: 2\n",
      "  PUNCT: 5\n",
      "  VERB: 5\n",
      "  DET: 3\n",
      "  ADJ: 3\n",
      "  PRON: 1\n",
      "  AUX: 2\n",
      "  ADV: 6\n",
      "===================================\n",
      "TEXTO: Poema XXVI — Dulce María Loynaz\n",
      "Tokens totales: 106\n",
      "Lemas totales : 50\n",
      "Distribución POS:\n",
      "  PUNCT: 14\n",
      "  AUX: 6\n",
      "  ADP: 25\n",
      "  VERB: 8\n",
      "  PRON: 7\n",
      "  SCONJ: 4\n",
      "  DET: 14\n",
      "  NOUN: 21\n",
      "  ADV: 5\n",
      "  CCONJ: 1\n",
      "  ADJ: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['“',\n",
       "  'He',\n",
       "  'de',\n",
       "  'amoldarme',\n",
       "  'a',\n",
       "  'ti',\n",
       "  'como',\n",
       "  'el',\n",
       "  'río',\n",
       "  'a',\n",
       "  'su',\n",
       "  'cauce',\n",
       "  ',',\n",
       "  'como',\n",
       "  'el',\n",
       "  'mar',\n",
       "  'a',\n",
       "  'su',\n",
       "  'playa',\n",
       "  ',',\n",
       "  'como',\n",
       "  'la',\n",
       "  'espada',\n",
       "  'a',\n",
       "  'su',\n",
       "  'vaina',\n",
       "  '.',\n",
       "  'He',\n",
       "  'de',\n",
       "  'correr',\n",
       "  'en',\n",
       "  'ti',\n",
       "  ',',\n",
       "  'he',\n",
       "  'de',\n",
       "  'cantar',\n",
       "  'en',\n",
       "  'ti',\n",
       "  ',',\n",
       "  'he',\n",
       "  'de',\n",
       "  'guardarme',\n",
       "  'en',\n",
       "  'ti',\n",
       "  'ya',\n",
       "  'para',\n",
       "  'siempre',\n",
       "  '.',\n",
       "  'Fuera',\n",
       "  'de',\n",
       "  'ti',\n",
       "  'ha',\n",
       "  'de',\n",
       "  'sobrarme',\n",
       "  'el',\n",
       "  'mundo',\n",
       "  'como',\n",
       "  'le',\n",
       "  'sobra',\n",
       "  'al',\n",
       "  'río',\n",
       "  'el',\n",
       "  'aire',\n",
       "  ',',\n",
       "  'al',\n",
       "  'mar',\n",
       "  'la',\n",
       "  'tierra',\n",
       "  ',',\n",
       "  'a',\n",
       "  'la',\n",
       "  'espada',\n",
       "  'la',\n",
       "  'mesa',\n",
       "  'del',\n",
       "  'convite',\n",
       "  '.',\n",
       "  'Dentro',\n",
       "  'de',\n",
       "  'ti',\n",
       "  'no',\n",
       "  'ha',\n",
       "  'de',\n",
       "  'faltarme',\n",
       "  'blandura',\n",
       "  'de',\n",
       "  'limo',\n",
       "  'para',\n",
       "  'mi',\n",
       "  'corriente',\n",
       "  ',',\n",
       "  'perfil',\n",
       "  'de',\n",
       "  'viento',\n",
       "  'para',\n",
       "  'mis',\n",
       "  'olas',\n",
       "  ',',\n",
       "  'ceñidura',\n",
       "  'y',\n",
       "  'reposo',\n",
       "  'para',\n",
       "  'mi',\n",
       "  'acero',\n",
       "  '.',\n",
       "  '”'],\n",
       " ['“',\n",
       "  'haber',\n",
       "  'de',\n",
       "  'amoldar yo',\n",
       "  'a',\n",
       "  'tú',\n",
       "  'como',\n",
       "  'el',\n",
       "  'río',\n",
       "  'a',\n",
       "  'su',\n",
       "  'cauce',\n",
       "  ',',\n",
       "  'como',\n",
       "  'el',\n",
       "  'mar',\n",
       "  'a',\n",
       "  'su',\n",
       "  'playa',\n",
       "  ',',\n",
       "  'como',\n",
       "  'el',\n",
       "  'espada',\n",
       "  'a',\n",
       "  'su',\n",
       "  'vaina',\n",
       "  '.',\n",
       "  'haber',\n",
       "  'de',\n",
       "  'correr',\n",
       "  'en',\n",
       "  'tú',\n",
       "  ',',\n",
       "  'haber',\n",
       "  'de',\n",
       "  'cantar',\n",
       "  'en',\n",
       "  'tú',\n",
       "  ',',\n",
       "  'haber',\n",
       "  'de',\n",
       "  'guardarme',\n",
       "  'en',\n",
       "  'tú',\n",
       "  'ya',\n",
       "  'para',\n",
       "  'siempre',\n",
       "  '.',\n",
       "  'fuera',\n",
       "  'de',\n",
       "  'tú',\n",
       "  'haber',\n",
       "  'de',\n",
       "  'sobrar yo',\n",
       "  'el',\n",
       "  'mundo',\n",
       "  'como',\n",
       "  'él',\n",
       "  'sobrar',\n",
       "  'al',\n",
       "  'río',\n",
       "  'el',\n",
       "  'aire',\n",
       "  ',',\n",
       "  'al',\n",
       "  'mar',\n",
       "  'el',\n",
       "  'tierra',\n",
       "  ',',\n",
       "  'a',\n",
       "  'el',\n",
       "  'espada',\n",
       "  'el',\n",
       "  'mesa',\n",
       "  'del',\n",
       "  'convite',\n",
       "  '.',\n",
       "  'dentro',\n",
       "  'de',\n",
       "  'tú',\n",
       "  'no',\n",
       "  'haber',\n",
       "  'de',\n",
       "  'faltar yo',\n",
       "  'blandura',\n",
       "  'de',\n",
       "  'limo',\n",
       "  'para',\n",
       "  'mi',\n",
       "  'corriente',\n",
       "  ',',\n",
       "  'perfil',\n",
       "  'de',\n",
       "  'viento',\n",
       "  'para',\n",
       "  'mi',\n",
       "  'ola',\n",
       "  ',',\n",
       "  'ceñidura',\n",
       "  'y',\n",
       "  'reposo',\n",
       "  'para',\n",
       "  'mi',\n",
       "  'acero',\n",
       "  '.',\n",
       "  '”'],\n",
       " Counter({'ADP': 25,\n",
       "          'NOUN': 21,\n",
       "          'PUNCT': 14,\n",
       "          'DET': 14,\n",
       "          'VERB': 8,\n",
       "          'PRON': 7,\n",
       "          'AUX': 6,\n",
       "          'ADV': 5,\n",
       "          'SCONJ': 4,\n",
       "          'CCONJ': 1,\n",
       "          'ADJ': 1}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "# Cargar modelo de spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Función auxiliar\n",
    "def procesar_texto(ruta_archivo, titulo):\n",
    "    with open(ruta_archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "        texto = f.read()\n",
    "    \n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    # Extraer tokens, lemas y POS\n",
    "    tokens = [token.text for token in doc if not token.is_space]\n",
    "    lemas = [token.lemma_ for token in doc if not token.is_space]\n",
    "    pos_tags = [token.pos_ for token in doc if not token.is_space]\n",
    "    \n",
    "    # Contar categorías POS\n",
    "    conteo_pos = Counter(pos_tags)\n",
    "    \n",
    "    # Reporte\n",
    "    print(\"===================================\")\n",
    "    print(f\"TEXTO: {titulo}\")\n",
    "    print(\"Tokens totales:\", len(tokens))\n",
    "    print(\"Lemas totales :\", len(set(lemas)))\n",
    "    print(\"Distribución POS:\")\n",
    "    for cat, freq in conteo_pos.items():\n",
    "        print(f\"  {cat}: {freq}\")\n",
    "    \n",
    "    return tokens, lemas, conteo_pos\n",
    "\n",
    "\n",
    "# VEamos en los dos archivos\n",
    "procesar_texto(\"ultimaInocencia.txt\", \"La última inocencia — Alejandra Pizarnik\")\n",
    "procesar_texto(\"DulceMaríaLoynaz.txt\", \"Poema XXVI — Dulce María Loynaz\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
